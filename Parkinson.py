# -*- coding: utf-8 -*-
"""Copy of Parkinson.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kIxS0yuKnbGXM-llbtxnsaGMziQZ9FQb
"""

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split
# to build linear regression_model
from sklearn.linear_model import LinearRegression
# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
# I changed this part
!pip install mlxtend
import joblib
import sys
sys.modules['sklearn.externals.joblib'] = joblib
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

# Importing Libraries
import requests
import pandas as pd
from imblearn.over_sampling import SMOTE
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB as Naive_Bayes
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_classification
from xgboost import XGBClassifier
import joblib
from IPython.display import display
import pickle
df=pd.read_csv('/content/parkinsons dataset.csv')

df.head()

df.tail()

print('Number of Features In Dataset :', df.shape[1])
print('Number of Instances In Dataset : ', df.shape[0])

# Dropping The Name Column
df.drop(['name'], axis=1, inplace=True)

print('Number of Features In Dataset :', df.shape[1])
print('Number of Instances In Dataset : ', df.shape[0])

df.info()

df.describe()

df['status'] = df['status'].astype('uint8')

# Checking For Duplicate Rows In Dataset
print('Number of Duplicated Rows :',df.duplicated().sum())

# Checking For Missing Values In Dataset
df.isna().sum()

#Balance of Data
sns.countplot(x='status',data=df)

#Box Plot
fig,axes=plt.subplots(5,5,figsize=(15,15))
axes=axes.flatten()
for i in range(1,len(df.columns)-1):
    sns.boxplot(x='status',y=df.iloc[:,i],data=df,orient='v',ax=axes[i])
plt.tight_layout()
plt.show()

plt.rcParams['figure.figsize'] = (15, 4)
sns.pairplot(df,hue = 'status', vars = ['MDVP:Jitter(%)','MDVP:Jitter(Abs)','MDVP:RAP','MDVP:PPQ', 'Jitter:DDP'] )
plt.show()

fig, ax = plt.subplots(figsize=(20,20))
sns.heatmap(df.corr(),annot=True,ax=ax)

plt.rcParams['figure.figsize'] = (15, 4)
sns.pairplot(df,hue = 'status', vars = ['MDVP:Shimmer','MDVP:Shimmer(dB)','Shimmer:APQ3','Shimmer:APQ5','MDVP:APQ','Shimmer:DDA'] )
plt.show()

# Exploring Imabalance In Dataset
df['status'].value_counts()

X = df.drop(['status'], axis=1)
y = df['status']

print('Feature (X) Shape Before Balancing :', X.shape)
print('Target (y) Shape Before Balancing :', y.shape)

y=df['status']
cols=['MDVP:RAP','Jitter:DDP','DFA','NHR','MDVP:Fhi(Hz)','status']
x=df.drop(cols,axis=1)

from sklearn.preprocessing import MinMaxScaler
# Scaling features between -1 and 1 for mormalization
scaler = MinMaxScaler((-1,1))

# define X_features , Y_labels
X_features = scaler.fit_transform(X)
Y_labels = y

# splitting the dataset into traning and testing sets into 80 - 20
train_size=0.80
test_size=0.20
seed=5
from sklearn.model_selection import train_test_split
X_train , X_test , y_train , y_test = train_test_split(X_features, Y_labels , test_size=0.20, random_state=20)

logmodel = LogisticRegression()
logmodel.fit(X_train, y_train)
predlog = logmodel.predict(X_test)

print(classification_report(y_test, predlog))
print("Confusion Matrix:")
confusion_matrix(y_test, predlog)

y_pred_proba = logmodel.predict_proba(X_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

# Dumping Logistic Regression Model
pickle.dump(logmodel, open('lg.pkl','wb'))

# Naive Bayes
gnb = Naive_Bayes()
gnb.fit(X_train, y_train)
predgnb = gnb.predict(X_test)
print(classification_report(y_test, predgnb))

print("Confusion Matrix:")
confusion_matrix(y_test, predgnb)

# scores -check how efficiently labels are predicted
accuracy_testing = accuracy_score(y_test, predgnb)
print("Accuracy % :",accuracy_testing*100)

y_pred_proba = gnb.predict_proba(X_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

pickle.dump(gnb,open ('gnb.pkl','wb'))

import numpy as np
Ks = 10
mean_acc = []
ConfustionMx = [];
for n in range(2,Ks):
  #Train Model and Predict
  neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)
  yhat=neigh.predict(X_test)
  mean_acc.append(metrics.accuracy_score(y_test, yhat))
print('Neighbor Accuracy List')
print(mean_acc)

plt.plot(range(2,Ks),mean_acc,'g')
plt.ylabel('Accuracy ')
plt.xlabel('Number of Neighbours (K)')
plt.tight_layout()
plt.show()

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
predKNN = knn.predict(X_test)

y_pred_proba = knn.predict_proba(X_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

# Dumping KNN Classifier
pickle.dump(knn, open('knn_clf.pkl','wb'))

# Defining Parameter Dictionary
param_dict = {'max_depth': range(4,8), 'eta' : [0.1, 0.2, 0.3, 0.4, 0.5],
'reg_lambda' : [0.8, 0.9, 1, 1.1, 1.2],
'random_state': [300, 600, 900]}

import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, r2_score

# Assuming predlog, predgnb, predKNN, and y_test are defined elsewhere in your code

chart = {
    'Metric': ["Accuracy", "F1-Score", "Recall", "Precision", "R2-Score"],
    'LR': [
        accuracy_score(y_test, predlog),
        f1_score(y_test, predlog),
        recall_score(y_test, predlog),
        precision_score(y_test, predlog),
        r2_score(y_test, predlog)  # Assuming you have defined the R2 score properly
    ],
    'NB': [
        accuracy_score(y_test, predgnb),
        f1_score(y_test, predgnb),
        recall_score(y_test, predgnb),
        precision_score(y_test, predgnb),
        r2_score(y_test, predgnb)  # Assuming you have defined the R2 score properly
    ],
    'KNN': [
        accuracy_score(y_test, predKNN),
        f1_score(y_test, predKNN),
        recall_score(y_test, predKNN),
        precision_score(y_test, predKNN),
        r2_score(y_test, predKNN)  # Assuming you have defined the R2 score properly
    ]
}

chart = pd.DataFrame(chart)
print(chart)

from matplotlib import pyplot as plt
chart['NB'].plot(kind='hist', bins=20, title='NB')
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
chart['KNN'].plot(kind='hist', bins=20, title='KNN')
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
chart['LR'].plot(kind='hist', bins=20, title='LR')
plt.gca().spines[['top', 'right',]].set_visible(False)

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Initialize Logistic Regression and Naive Bayes classifiers
logmodel = LogisticRegression()
gnb = GaussianNB()

# Fit both models on training data
logmodel.fit(X_train, y_train)
gnb.fit(X_train, y_train)

# Predict probabilities for both models
log_prob = logmodel.predict_proba(X_test)[:, 1]
gnb_prob = gnb.predict_proba(X_test)[:, 1]

# Weighted average of probabilities
hybrid_prob = (log_prob + gnb_prob) / 2

# Compute accuracy for each model
log_accuracy = accuracy_score(y_test, logmodel.predict(X_test))
gnb_accuracy = accuracy_score(y_test, gnb.predict(X_test))

# Compute AUC for each model
log_auc = roc_auc_score(y_test, log_prob)
gnb_auc = roc_auc_score(y_test, gnb_prob)
hybrid_auc = roc_auc_score(y_test, hybrid_prob)

# Plot ROC curves for each model
plt.figure(figsize=(8, 6))
fpr_log, tpr_log, _ = roc_curve(y_test, log_prob)
fpr_gnb, tpr_gnb, _ = roc_curve(y_test, gnb_prob)
fpr_hybrid, tpr_hybrid, _ = roc_curve(y_test, hybrid_prob)
plt.plot(fpr_log, tpr_log, label='Logistic Regression (AUC = {:.2f}, Acc = {:.2f}%)'.format(log_auc, log_accuracy * 100))
plt.plot(fpr_gnb, tpr_gnb, label='Naive Bayes (AUC = {:.2f}, Acc = {:.2f}%)'.format(gnb_auc, gnb_accuracy * 100))
plt.plot(fpr_hybrid, tpr_hybrid, label='Hybrid (AUC = {:.2f})'.format(hybrid_auc), linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score
import matplotlib.pyplot as plt
# Initialize Naive Bayes and KNN classifiers
gnb = GaussianNB()
knn = KNeighborsClassifier(n_neighbors=5)
# Fit both models on training data
gnb.fit(X_train, y_train)
knn.fit(X_train, y_train)
# Predict probabilities for both models
gnb_prob = gnb.predict_proba(X_test)[:, 1]
knn_prob = knn.predict_proba(X_test)[:, 1]
# Weighted average of probabilities
hybrid_prob = (gnb_prob + knn_prob) / 2
# Compute accuracy for each model
gnb_accuracy = accuracy_score(y_test, gnb.predict(X_test))
knn_accuracy = accuracy_score(y_test, knn.predict(X_test))
# Compute AUC for each model
gnb_auc = roc_auc_score(y_test, gnb_prob)
knn_auc = roc_auc_score(y_test, knn_prob)
hybrid_auc = roc_auc_score(y_test, hybrid_prob)
# Plot ROC curves for each model
plt.figure(figsize=(8, 6))
fpr_gnb, tpr_gnb, _ = roc_curve(y_test, gnb_prob)
fpr_knn, tpr_knn, _ = roc_curve(y_test, knn_prob)
fpr_hybrid, tpr_hybrid, _ = roc_curve(y_test, hybrid_prob)
plt.plot(fpr_gnb, tpr_gnb, label='Naive Bayes (AUC = {:.2f}, Acc = {:.2f}%)'.format(gnb_auc, gnb_accuracy * 100))
plt.plot(fpr_knn, tpr_knn, label='KNN (AUC = {:.2f}, Acc = {:.2f}%)'.format(knn_auc, knn_accuracy * 100))
plt.plot(fpr_hybrid, tpr_hybrid, label='Hybrid (AUC = {:.2f})'.format(hybrid_auc), linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score
import matplotlib.pyplot as plt
# Initialize classifiers
logmodel = LogisticRegression()
gnb = GaussianNB()
knn = KNeighborsClassifier(n_neighbors=5)
# Fit all models on training data
logmodel.fit(X_train, y_train)
gnb.fit(X_train, y_train)
knn.fit(X_train, y_train)
# Predict probabilities for all models
log_prob = logmodel.predict_proba(X_test)[:, 1]
gnb_prob = gnb.predict_proba(X_test)[:, 1]
knn_prob = knn.predict_proba(X_test)[:, 1]
# Weighted average of probabilities
hybrid_prob = (log_prob + gnb_prob + knn_prob) / 3
# Compute accuracy for each model
log_accuracy = accuracy_score(y_test, logmodel.predict(X_test))
gnb_accuracy = accuracy_score(y_test, gnb.predict(X_test))
knn_accuracy = accuracy_score(y_test, knn.predict(X_test))
# Compute AUC for each model
log_auc = roc_auc_score(y_test, log_prob)
gnb_auc = roc_auc_score(y_test, gnb_prob)
knn_auc = roc_auc_score(y_test, knn_prob)
hybrid_auc = roc_auc_score(y_test, hybrid_prob)
# Plot ROC curves for each model
plt.figure(figsize=(8, 6))
fpr_log, tpr_log, _ = roc_curve(y_test, log_prob)
fpr_gnb, tpr_gnb, _ = roc_curve(y_test, gnb_prob)
fpr_knn, tpr_knn, _ = roc_curve(y_test, knn_prob)
fpr_hybrid, tpr_hybrid, _ = roc_curve(y_test, hybrid_prob)
plt.plot(fpr_log, tpr_log, label='Logistic Regression (AUC = {:.2f}, Acc = {:.2f}%)'.format(log_auc, log_accuracy * 100))
plt.plot(fpr_gnb, tpr_gnb, label='Naive Bayes (AUC = {:.2f}, Acc = {:.2f}%)'.format(gnb_auc, gnb_accuracy * 100))
plt.plot(fpr_knn, tpr_knn, label='KNN (AUC = {:.2f}, Acc = {:.2f}%)'.format(knn_auc, knn_accuracy * 100))
plt.plot(fpr_hybrid, tpr_hybrid, label='Hybrid (AUC = {:.2f})'.format(hybrid_auc), linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
# Assuming you have a DataFrame 'data' with multiple observations for each algorithm
# For demonstration purposes, I'll create a random dataset
import pandas as pd
import numpy as np
algorithms = ['Logistic Regression', 'Naive Bayes', 'KNN', 'Hybrid']
auc_scores = np.random.rand(100, 4) # Random AUC scores for 100 observations and 4 algorithms
# Create a DataFrame for visualization
data = pd.DataFrame(auc_scores, columns=algorithms)
# Plotting box plots for AUC scores
plt.figure(figsize=(10, 6))
sns.boxplot(data=data)
plt.title('AUC Scores Comparison')
plt.xlabel('Algorithm')
plt.ylabel('AUC')
plt.show()

#Random Foresst
# Importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, accuracy_score
import joblib  # For saving the model

# Load the dataset
df = pd.read_csv('/content/parkinsons dataset.csv')

# Dropping the 'name' column as it's not needed for model building
df.drop(['name'], axis=1, inplace=True)

# Convert the target variable 'status' to integer
df['status'] = df['status'].astype('uint8')

# Splitting the dataset into features (X) and target (y)
X = df.drop(['status'], axis=1)
y = df['status']

# Normalize the feature values
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler((-1, 1))
X_scaled = scaler.fit_transform(X)

# Splitting the dataset into training and testing sets (80%-20%)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model on the training set
rf_model.fit(X_train, y_train)

# Save the trained Random Forest model to a .pkl file
model_filename = "random_forest_model.pkl"
joblib.dump(rf_model, model_filename)
print(f"Model saved as {model_filename}")

# Make predictions on the test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model's performance
print("Classification Report:\n", classification_report(y_test, y_pred_rf))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
print("Accuracy: {:.2f}%".format(accuracy_score(y_test, y_pred_rf) * 100))

# Predict probabilities for ROC curve
y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]

# Calculate AUC and plot ROC curve
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)
auc_rf = roc_auc_score(y_test, y_pred_proba_rf)

plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, label='Random Forest (AUC = {:.2f})'.format(auc_rf), color='blue')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Random Forest')
plt.legend()
plt.grid()
plt.show()

# Feature Importance Visualization
importances = rf_model.feature_importances_
feature_names = X.columns

# Creating a DataFrame for better visualization
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plotting feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Feature Importance for Random Forest')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

#Support Vector Machine(SVM)
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import RFE
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE  # For balancing the dataset
import matplotlib.pyplot as plt
import joblib  # For saving the model

# Load the dataset
data = pd.read_csv('parkinsons dataset.csv')

# Separate features and target variable
X = data.drop(columns=['status', 'name'])  # Features
y = data['status']  # Target variable

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Balance the dataset using SMOTE
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Normalize the feature values
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Feature selection using Recursive Feature Elimination (RFE)
selector = RFE(estimator=SVC(kernel='linear'), n_features_to_select=10)
X_train = selector.fit_transform(X_train, y_train)
X_test = selector.transform(X_test)

# Hyperparameter tuning using GridSearchCV
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [1, 0.1, 0.01, 0.001],
    'kernel': ['rbf', 'linear', 'poly', 'sigmoid']
}
grid_search = GridSearchCV(SVC(probability=True), param_grid, cv=5, scoring='accuracy', verbose=2)
grid_search.fit(X_train, y_train)

# Get the best parameters and model
best_params = grid_search.best_params_
print("\nBest Parameters:", best_params)

# Train the SVM model with optimal parameters
svm_model = SVC(kernel=best_params['kernel'], C=best_params['C'], gamma=best_params['gamma'], probability=True)
svm_model.fit(X_train, y_train)

# Save the trained model to a .pkl file
model_filename = "svm_model.pkl"
joblib.dump(svm_model, model_filename)
print(f"\nModel saved as {model_filename}")

# Evaluate using cross-validation
cv_scores = cross_val_score(svm_model, X_train, y_train, cv=5)
print("\nCross-Validation Accuracy:", np.mean(cv_scores))

# Make predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

accuracy = accuracy_score(y_test, y_pred)
print("\nAccuracy Score:", accuracy)

# Calculate ROC-AUC
y_proba = svm_model.predict_proba(X_test)[:, 1]
roc_auc = roc_auc_score(y_test, y_proba)
print("\nROC-AUC Score:", roc_auc)

# Plot the ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"SVM (AUC = {roc_auc:.2f})", color='blue')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for SVM')
plt.legend(loc='best')
plt.grid()
plt.show()

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import joblib  # For saving the model

# Load the dataset
data = pd.read_csv('parkinsons dataset.csv')

# Separate features and target variable
# Drop both 'status' and 'name' columns for features
X = data.drop(columns=['status', 'name'])  # Features
y = data['status']  # Target variable

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 1: Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 2: Train Random Forest for feature importance
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)

# Extract feature importance
feature_importance = rf_model.feature_importances_
important_features = np.argsort(feature_importance)[-10:]  # Top 10 features

# Reduce dataset to important features
X_train_selected = X_train_scaled[:, important_features]
X_test_selected = X_test_scaled[:, important_features]

# Step 3: Train Logistic Regression using selected features
logreg_model = LogisticRegression(random_state=42)
logreg_model.fit(X_train_selected, y_train)

# Evaluate the hybrid model
y_pred = logreg_model.predict(X_test_selected)
y_proba = logreg_model.predict_proba(X_test_selected)[:, 1]

# Evaluation metrics
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

accuracy = accuracy_score(y_test, y_pred)
print("\nAccuracy Score:", accuracy)

roc_auc = roc_auc_score(y_test, y_proba)
print("\nROC-AUC Score:", roc_auc)

# Plot the ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"Hybrid (RF + LR) (AUC = {roc_auc:.2f})", color='blue')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='best')
plt.grid()
plt.show()

# Step 4: Save the hybrid model
class HybridModel:
    def __init__(self, rf_model, logreg_model, scaler, important_features):
        self.rf_model = rf_model
        self.logreg_model = logreg_model
        self.scaler = scaler
        self.important_features = important_features

    def predict(self, X):
        # Apply scaler
        X_scaled = self.scaler.transform(X)
        # Select important features
        X_selected = X_scaled[:, self.important_features]
        # Predict using Logistic Regression
        return self.logreg_model.predict(X_selected)

    def predict_proba(self, X):
        # Apply scaler
        X_scaled = self.scaler.transform(X)
        # Select important features
        X_selected = X_scaled[:, self.important_features]
        # Predict probabilities using Logistic Regression
        return self.logreg_model.predict_proba(X_selected)

# Create and save the hybrid model
hybrid_model = HybridModel(rf_model, logreg_model, scaler, important_features)
joblib.dump(hybrid_model, 'hybrid_model_rf_lr.pkl')
print("\nHybrid model saved as 'hybrid_model_rf_lr.pkl'")

# Step 5: Load and validate the saved hybrid model
loaded_hybrid_model = joblib.load('hybrid_model_rf_lr.pkl')
y_loaded_pred = loaded_hybrid_model.predict(X_test)
y_loaded_proba = loaded_hybrid_model.predict_proba(X_test)[:, 1]

# Validate loaded model's performance
print("\nLoaded Model Validation:")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_loaded_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_loaded_pred))

loaded_accuracy = accuracy_score(y_test, y_loaded_pred)
print("\nLoaded Model Accuracy Score:", loaded_accuracy)

loaded_roc_auc = roc_auc_score(y_test, y_loaded_proba)
print("\nLoaded Model ROC-AUC Score:", loaded_roc_auc)

# Plot the ROC Curve for the loaded model
fpr_loaded, tpr_loaded, thresholds_loaded = roc_curve(y_test, y_loaded_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr_loaded, tpr_loaded, label=f"Loaded Hybrid Model (AUC = {loaded_roc_auc:.2f})", color='green')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Loaded Model')
plt.legend(loc='best')
plt.grid()
plt.show()

#fully connected feedforward neural network (also known as a dense neural network).
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt  # Import matplotlib for plotting

# Load the dataset
df = pd.read_csv('parkinsons dataset.csv')

# Dropping the 'name' column
df.drop(['name'], axis=1, inplace=True)

# Convert the target variable 'status' to integer
df['status'] = df['status'].astype('uint8')

# Splitting the dataset into features (X) and target (y)
X = df.drop(['status'], axis=1)
y = df['status']

# Normalize the feature values
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Splitting the dataset into training and testing sets (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Build a simple neural network model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # For binary classification
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model and store the training history
history = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_split=0.2)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy:.2f}')

# Save the trained model to a file
model.save('neural_network_model.h5')

# Plot training history
plt.figure(figsize=(12, 4))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()

# Install compatible versions
!pip install scikit-learn==0.24.2
!pip install xgboost==1.5.1

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import xgboost as xgb
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import joblib  # For saving the model

# Load the dataset
df = pd.read_csv('parkinsons dataset.csv')

# Dropping the 'name' column
df.drop(['name'], axis=1, inplace=True)

# Convert the target variable 'status' to integer
df['status'] = df['status'].astype('uint8')

# Splitting the dataset into features (X) and target (y)
X = df.drop(['status'], axis=1)
y = df['status']

# Normalize the feature values
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Splitting the dataset into training and testing sets (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Build and train the XGBoost model
xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    random_state=42
)

# Train the model
xgb_model.fit(X_train, y_train)

# Save the trained model
model_filename = 'xgboost_model.pkl'
joblib.dump(xgb_model, model_filename)
print(f"Model saved as {model_filename}")

# Make predictions on the test set
y_pred = xgb_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy Score: {accuracy:.2f}")

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Calculate ROC-AUC
y_proba = xgb_model.predict_proba(X_test)[:, 1]
roc_auc = roc_auc_score(y_test, y_proba)
print(f"\nROC-AUC Score: {roc_auc:.2f}")

# Plot the ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"XGBoost (AUC = {roc_auc:.2f})", color='blue')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for XGBoost')
plt.legend(loc='best')
plt.grid()
plt.show()

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import xgboost as xgb

# Load the dataset
df = pd.read_csv('parkinsons dataset.csv')

# Dropping the 'name' column
df.drop(['name'], axis=1, inplace=True)

# Convert the target variable 'status' to integer
df['status'] = df['status'].astype('uint8')

# Splitting the dataset into features (X) and target (y)
X = df.drop(['status'], axis=1)
y = df['status']

# Normalize the feature values
scaler = MinMaxScaler((-1, 1))
X_scaled = scaler.fit_transform(X)

# Splitting the dataset into training and testing sets (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize models
logmodel = LogisticRegression()
gnb = GaussianNB()
knn = KNeighborsClassifier(n_neighbors=5)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
svm_model = SVC(probability=True)
xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    random_state=42
)

# Train models
logmodel.fit(X_train, y_train)
gnb.fit(X_train, y_train)
knn.fit(X_train, y_train)
rf_model.fit(X_train, y_train)
svm_model.fit(X_train, y_train)
xgb_model.fit(X_train, y_train)

# Make predictions
y_pred_log = logmodel.predict(X_test)
y_pred_gnb = gnb.predict(X_test)
y_pred_knn = knn.predict(X_test)
y_pred_rf = rf_model.predict(X_test)
y_pred_svm = svm_model.predict(X_test)
y_pred_xgb = xgb_model.predict(X_test)

# Calculate accuracy for each model
log_accuracy = accuracy_score(y_test, y_pred_log)
gnb_accuracy = accuracy_score(y_test, y_pred_gnb)
knn_accuracy = accuracy_score(y_test, y_pred_knn)
rf_accuracy = accuracy_score(y_test, y_pred_rf)
svm_accuracy = accuracy_score(y_test, y_pred_svm)
xgb_accuracy = accuracy_score(y_test, y_pred_xgb)

# Neural Network model's accuracy (manually set)
nn_accuracy = 0.95

# Store accuracy scores in a dictionary
accuracy_scores = {
    'Logistic Regression': log_accuracy,
    'Naive Bayes': gnb_accuracy,
    'KNN': knn_accuracy,
    'Random Forest': rf_accuracy,
    'SVM': svm_accuracy,
    'XGBoost': xgb_accuracy,
    'Neural Network': nn_accuracy
}

# Extracting model names and their corresponding accuracy scores
models = list(accuracy_scores.keys())
scores = list(accuracy_scores.values())

# Creating the bar plot
plt.figure(figsize=(12, 6))
bars = plt.bar(models, scores, color=['skyblue', 'salmon', 'lightgreen', 'orange', 'lightcoral', 'gold', 'purple'])

# Adding accuracy values on top of the bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')

# Adding titles and labels
plt.title('Model Accuracy Comparison')
plt.xlabel('Models')
plt.ylabel('Accuracy (%)')
plt.ylim(0, 1)  # Assuming accuracy is between 0 and 1
plt.xticks(rotation=45)
plt.grid(axis='y')

# Show the plot
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv('parkinsons dataset.csv')

# Dropping the 'name' column
df.drop(['name'], axis=1, inplace=True)

# Convert the target variable 'status' to integer
df['status'] = df['status'].astype('uint8')

# Splitting the dataset into features (X) and target (y)
X = df.drop(['status'], axis=1)
y = df['status']

# Normalize the feature values
scaler = MinMaxScaler((-1, 1))
X_scaled = scaler.fit_transform(X)

# Splitting the dataset into training and testing sets (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize models
logmodel = LogisticRegression()
gnb = GaussianNB()
knn = KNeighborsClassifier(n_neighbors=5)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train models
logmodel.fit(X_train, y_train)
gnb.fit(X_train, y_train)
knn.fit(X_train, y_train)
rf_model.fit(X_train, y_train)

# Make predictions for hybrid model 1 (Logistic Regression + Naive Bayes)
y_pred_log = logmodel.predict(X_test)
y_pred_gnb = gnb.predict(X_test)
hybrid1_pred = (y_pred_log + y_pred_gnb) / 2  # Average predictions
hybrid1_pred = np.round(hybrid1_pred).astype(int)  # Convert to binary predictions

# Calculate accuracy for hybrid model 1
hybrid1_accuracy = accuracy_score(y_test, hybrid1_pred)

# Make predictions for hybrid model 2 (Naive Bayes + KNN)
y_pred_knn = knn.predict(X_test)
hybrid2_pred = (y_pred_gnb + y_pred_knn) / 2  # Average predictions
hybrid2_pred = np.round(hybrid2_pred).astype(int)  # Convert to binary predictions

# Calculate accuracy for hybrid model 2
hybrid2_accuracy = accuracy_score(y_test, hybrid2_pred)

# Make predictions for hybrid model 3 (Logistic Regression + Naive Bayes + KNN)
hybrid3_pred = (y_pred_log + y_pred_gnb + y_pred_knn) / 3  # Average predictions
hybrid3_pred = np.round(hybrid3_pred).astype(int)  # Convert to binary predictions

# Calculate accuracy for hybrid model 3
hybrid3_accuracy = accuracy_score(y_test, hybrid3_pred)

# Make predictions for hybrid model 4 (Logistic Regression + KNN)
hybrid4_pred = (y_pred_log + y_pred_knn) / 2  # Average predictions
hybrid4_pred = np.round(hybrid4_pred).astype(int)  # Convert to binary predictions

# Calculate accuracy for hybrid model 4
hybrid4_accuracy = accuracy_score(y_test, hybrid4_pred)

# Make predictions for hybrid model 5 (Random Forest + Logistic Regression)
y_pred_rf = rf_model.predict(X_test)
hybrid5_pred = (y_pred_rf + y_pred_log) / 2  # Average predictions
hybrid5_pred = np.round(hybrid5_pred).astype(int)  # Convert to binary predictions

# Calculate accuracy for hybrid model 5
hybrid5_accuracy = accuracy_score(y_test, hybrid5_pred)

# Store accuracy scores in a dictionary
hybrid_accuracy_scores = {
    'LR + NB': hybrid1_accuracy,
    'NB + KNN': hybrid2_accuracy,
    'LR + NBayes + KNN': hybrid3_accuracy,
    'LR + KNN': hybrid4_accuracy,
    'RF + LR': hybrid5_accuracy  # New hybrid model
}

# Extracting model names and their corresponding accuracy scores
hybrid_models = list(hybrid_accuracy_scores.keys())
hybrid_scores = list(hybrid_accuracy_scores.values())

# Creating the bar plot for hybrid models
plt.figure(figsize=(10,  6))
bars = plt.bar(hybrid_models, hybrid_scores, color=['lightblue', 'lightgreen', 'salmon', 'orange', 'purple'])

# Adding accuracy values on top of the bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')

# Adding titles and labels
plt.title('Hybrid Model Accuracy Comparison')
plt.xlabel('Hybrid Models')
plt.ylabel('Accuracy')
plt.ylim(0, 1)

# Adjust layout to avoid overlaps
plt.xticks(rotation=15)  # Rotate x-axis labels for better visibility
plt.tight_layout()  # Ensures proper spacing
plt.show()